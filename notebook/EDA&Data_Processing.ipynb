{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "93a9bff7",
      "metadata": {
        "id": "93a9bff7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "use_cols = ['user_id', 'item_id', 'timestamp', 'click', 'like', 'follow', 'search']\n",
        "df = pd.read_csv('rec_inter.csv', usecols=use_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "323d5068",
      "metadata": {
        "id": "323d5068"
      },
      "source": [
        "## data cleaning & preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c04fa7e",
      "metadata": {
        "id": "7c04fa7e"
      },
      "outputs": [],
      "source": [
        "# data cleaning & preprocessing\n",
        "\n",
        "for c in [\"click\",\"like\",\"follow\",\"search\"]:\n",
        "    df[c] = df[c].fillna(0).astype(np.int8)\n",
        "\n",
        "# only keep recommendation interaction(none search)\n",
        "df = df[df['search'] == 0]\n",
        "\n",
        "df['pos'] = ((df['click'] + df['like'] + df['follow']) > 0).astype(np.int8)\n",
        "\n",
        "# timestamp\n",
        "ts = pd.to_numeric(df['timestamp'], errors='coerce')\n",
        "df = df[ts.notna()].copy()\n",
        "df['ts'] = ts.astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "80c243c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80c243c0",
        "outputId": "8cb41fb8-5acd-4347-aa22-160d294269a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: 7461153 interactions\n",
            "Filtered: 2621943 interactions\n",
            "Users: 23926, Items: 50000\n",
            "Train: 2598017, Test: 23926\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. Iterative Filtering (Core Stability)\n",
        "# ==========================================\n",
        "# Keep top K items to avoid OOM and reduce noise\n",
        "target_item_count = 50000\n",
        "min_user_inter = 5\n",
        "\n",
        "print(f\"Original: {len(df)} interactions\")\n",
        "\n",
        "# Filter Items first (Keep Top 50k)\n",
        "item_counts = df['item_id'].value_counts()\n",
        "if len(item_counts) > target_item_count:\n",
        "    top_items = item_counts.head(target_item_count).index\n",
        "    df_filtered = df[df['item_id'].isin(top_items)].copy()\n",
        "else:\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "# Filter Users (Keep >= 5 interactions)\n",
        "# We might need a loop because removing users might reduce item counts, and vice versa\n",
        "# But for simplicity, one pass usually works well enough for coursework\n",
        "user_counts = df_filtered['user_id'].value_counts()\n",
        "valid_users = user_counts[user_counts >= min_user_inter].index\n",
        "df_filtered = df_filtered[df_filtered['user_id'].isin(valid_users)].copy()\n",
        "\n",
        "print(f\"Filtered: {len(df_filtered)} interactions\")\n",
        "print(f\"Users: {df_filtered['user_id'].nunique()}, Items: {df_filtered['item_id'].nunique()}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. ID Remapping\n",
        "# ==========================================\n",
        "unique_users = df_filtered['user_id'].unique()\n",
        "unique_items = df_filtered['item_id'].unique()\n",
        "\n",
        "user2idx = {uid: i for i, uid in enumerate(unique_users)}\n",
        "item2idx = {iid: i for i, iid in enumerate(unique_items)}\n",
        "\n",
        "df_filtered['user_idx'] = df_filtered['user_id'].map(user2idx)\n",
        "df_filtered['item_idx'] = df_filtered['item_id'].map(item2idx)\n",
        "\n",
        "num_users = len(unique_users)\n",
        "num_items = len(unique_items)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Train/Test Split\n",
        "# ==========================================\n",
        "df_filtered = df_filtered.sort_values(['user_idx', 'ts'])\n",
        "grouped = df_filtered.groupby('user_idx')\n",
        "test = df_filtered.loc[grouped.tail(1).index]\n",
        "train = df_filtered.drop(test.index)\n",
        "\n",
        "print(f\"Train: {len(train)}, Test: {len(test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f5735b",
      "metadata": {
        "id": "92f5735b"
      },
      "source": [
        "## Most Popular Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "af6fc569",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af6fc569",
        "outputId": "20d7ebc3-cd6e-4cf2-c318-45433c83107b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MostPopular] HR@50: 0.0303  NDCG@50: 0.0089\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. Define Reusable Evaluation Function\n",
        "# ==========================================\n",
        "def evaluate_model(model_name, test_df, topk_preds, K=50):\n",
        "    \"\"\"\n",
        "    test_df: DataFrame with 'user_idx' and 'item_idx' (ground truth)\n",
        "    topk_preds: dict or Series, user_idx -> list of top K item_indices\n",
        "    \"\"\"\n",
        "    hits = []\n",
        "    ndcgs = []\n",
        "\n",
        "    # Convert predictions to a dict for fast lookup if it isn't already\n",
        "    if not isinstance(topk_preds, dict):\n",
        "        pred_dict = topk_preds.to_dict()\n",
        "    else:\n",
        "        pred_dict = topk_preds\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        u = row['user_idx']\n",
        "        gt = row['item_idx']\n",
        "\n",
        "        # Get recommendations for this user, default to empty if missing\n",
        "        recs = pred_dict.get(u, [])\n",
        "\n",
        "        # HR@K\n",
        "        if gt in recs:\n",
        "            hits.append(1)\n",
        "            # NDCG@K\n",
        "            rank = recs.index(gt)\n",
        "            ndcgs.append(1.0 / np.log2(rank + 2))\n",
        "        else:\n",
        "            hits.append(0)\n",
        "            ndcgs.append(0.0)\n",
        "\n",
        "    hr = np.mean(hits)\n",
        "    ndcg = np.mean(ndcgs)\n",
        "    print(f\"[{model_name}] HR@{K}: {hr:.4f}  NDCG@{K}: {ndcg:.4f}\")\n",
        "    return hr, ndcg\n",
        "\n",
        "# ==========================================\n",
        "# 2. Run Most Popular Baseline\n",
        "# ==========================================\n",
        "# Calculate popularity on TRAIN set only (avoid data leakage)\n",
        "# Using weighted popularity as you did before\n",
        "train['w'] = (1*train['click'] + 2*train['like'] + 3*train['follow']).astype(np.int16)\n",
        "pop_scores = train.groupby('item_idx')['w'].sum().sort_values(ascending=False)\n",
        "\n",
        "# Get global Top-K list\n",
        "K = 50\n",
        "global_topk = pop_scores.index[:K].tolist()\n",
        "\n",
        "# Assign same topk to all test users\n",
        "most_pop_preds = {u: global_topk for u in test['user_idx'].unique()}\n",
        "\n",
        "# Evaluate\n",
        "hr_MP, ndcg_MP = evaluate_model(\"MostPopular\", test, most_pop_preds, K=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aa83f2c",
      "metadata": {
        "id": "7aa83f2c"
      },
      "source": [
        "## Item CF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ba2df237",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba2df237",
        "outputId": "8df4b0ae-09af-40c1-f1e7-ab90101b25d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1095330765.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  indices = torch.tensor([train['user_idx'].values, train['item_idx'].values], dtype=torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparse User-Item Matrix Shape: torch.Size([23926, 50000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Setup Device (GPU/CPU)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using CUDA GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Prepare Indices and Values from Train Data\n",
        "indices = torch.tensor([train['user_idx'].values, train['item_idx'].values], dtype=torch.long)\n",
        "values = torch.tensor(train['w'].values, dtype=torch.float32)\n",
        "shape = torch.Size((num_users, num_items))\n",
        "\n",
        "\n",
        "# Construct User-Item Sparse Matrix\n",
        "# user_item_mat: (Users x Items)\n",
        "user_item_mat = torch.sparse_coo_tensor(indices, values, shape, device=device)\n",
        "print(f\"Sparse User-Item Matrix Shape: {user_item_mat.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cfb0d195",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfb0d195",
        "outputId": "adcaf5b3-5b36-4adb-db04-3fbd353975c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0418, 0.0415,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0418, 0.0000, 0.0805,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0415, 0.0805, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        ...,\n",
              "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Calculate Item-Item Similarity (Cosine)\n",
        "\n",
        "# Transpose to (Items x Users)\n",
        "item_user_mat = user_item_mat.t()\n",
        "\n",
        "item_user_dense = item_user_mat.to_dense()\n",
        "item_norms = torch.norm(item_user_dense, p=2, dim=1, keepdim=True)\n",
        "item_norms[item_norms == 0] = 1e-9\n",
        "item_user_norm = item_user_dense / item_norms\n",
        "sim_matrix = torch.mm(item_user_norm, item_user_norm.t()) # (Items x Items)\n",
        "\n",
        "sim_matrix.fill_diagonal_(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b784c531",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b784c531",
        "outputId": "196d9f44-0944-4998-d698-32f43dc1899f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 120/120 [00:07<00:00, 15.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ItemCF] HR@50: 0.0548  NDCG@50: 0.0160\n"
          ]
        }
      ],
      "source": [
        "# Generate Recommendations\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# 5. Generate Recommendations\n",
        "item_cf_preds = {}\n",
        "K = 50\n",
        "test_users_arr = test['user_idx'].unique()\n",
        "batch_size = 200 # Adjust based on memory\n",
        "\n",
        "for i in tqdm(range(0, len(test_users_arr), batch_size), desc=\"Predicting\"):\n",
        "    batch_uids = test_users_arr[i : i + batch_size]\n",
        "    batch_uids_tensor = torch.tensor(batch_uids, device=device)\n",
        "\n",
        "    # Get History: (Batch x Items)\n",
        "    # index_select works on dense or sparse (if supported)\n",
        "    # converting batch history to dense for calculation\n",
        "    batch_hist = user_item_mat.index_select(0, batch_uids_tensor).to_dense()\n",
        "\n",
        "    # Score: (Batch x Items) * (Items x Items) -> (Batch x Items)\n",
        "    scores = torch.mm(batch_hist, sim_matrix)\n",
        "\n",
        "    # Mask seen items\n",
        "    scores = scores - 9999.0 * batch_hist\n",
        "\n",
        "    # Top-K\n",
        "    _, topk_indices = torch.topk(scores, k=K, dim=1)\n",
        "\n",
        "    # Store\n",
        "    topk_cpu = topk_indices.cpu().numpy()\n",
        "    for idx, u in enumerate(batch_uids):\n",
        "        item_cf_preds[u] = topk_cpu[idx].tolist()\n",
        "\n",
        "# 6. Evaluate\n",
        "hr_IC, ndcg_IC = evaluate_model(\"ItemCF\", test, item_cf_preds, K=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4dd22d8",
      "metadata": {
        "id": "d4dd22d8"
      },
      "source": [
        "## NeuCF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "57243171",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57243171",
        "outputId": "64561938-de1a-4fda-f53b-33524e5b8da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuMF Dataset Ready.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "# ==========================================\n",
        "# 1. NeuMF Dataset with Negative Sampling\n",
        "# ==========================================\n",
        "class NeuMFDataset(Dataset):\n",
        "    def __init__(self, train_df, num_items, num_neg=4):\n",
        "        self.users = torch.tensor(train_df['user_idx'].values, dtype=torch.long)\n",
        "        self.items = torch.tensor(train_df['item_idx'].values, dtype=torch.long)\n",
        "        self.num_items = num_items\n",
        "        self.num_neg = num_neg\n",
        "\n",
        "        # Pre-compute a set of interacted items for each user for fast negative sampling\n",
        "        # set lookup is O(1)\n",
        "        self.user_item_set = train_df.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Positive sample\n",
        "        u = self.users[idx]\n",
        "        i = self.items[idx]\n",
        "\n",
        "        # Negative sampling\n",
        "        # We need to find an item 'j' that user 'u' has NOT seen\n",
        "        item_indices = []\n",
        "        labels = []\n",
        "\n",
        "        # Add Positive\n",
        "        item_indices.append(i)\n",
        "        labels.append(1.0)\n",
        "\n",
        "        # Add Negatives\n",
        "        interacted_items = self.user_item_set.get(u.item(), set())\n",
        "\n",
        "        for _ in range(self.num_neg):\n",
        "            j = random.randint(0, self.num_items - 1)\n",
        "            # Simple rejection sampling\n",
        "            while j in interacted_items:\n",
        "                j = random.randint(0, self.num_items - 1)\n",
        "\n",
        "            item_indices.append(torch.tensor(j, dtype=torch.long))\n",
        "            labels.append(0.0)\n",
        "\n",
        "        return u, torch.stack(item_indices), torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "# num_neg=4 is standard for NCF/NeuMF\n",
        "train_dataset = NeuMFDataset(train, num_items=num_items, num_neg=4)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "\n",
        "print(\"NeuMF Dataset Ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "82e94d9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82e94d9a",
        "outputId": "b38b144f-d6df-42fe-b34e-be769da10863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuMF(\n",
            "  (gmf_user_embed): Embedding(23926, 32)\n",
            "  (gmf_item_embed): Embedding(50000, 32)\n",
            "  (mlp_user_embed): Embedding(23926, 32)\n",
            "  (mlp_item_embed): Embedding(50000, 32)\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (output): Linear(in_features=48, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 2. NeuMF Model Architecture\n",
        "# ==========================================\n",
        "class NeuMF(nn.Module):\n",
        "    def __init__(self, num_users, num_items, embedding_dim=32):\n",
        "        super(NeuMF, self).__init__()\n",
        "\n",
        "        # --- GMF Part ---\n",
        "        self.gmf_user_embed = nn.Embedding(num_users, embedding_dim)\n",
        "        self.gmf_item_embed = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        # --- MLP Part ---\n",
        "        self.mlp_user_embed = nn.Embedding(num_users, embedding_dim)\n",
        "        self.mlp_item_embed = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        # MLP Layers: Input(2*dim) -> dim -> dim/2 -> 1\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim * 2, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(embedding_dim, embedding_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # --- Prediction Layer ---\n",
        "        # Concatenate GMF output (dim) and MLP output (dim/2)\n",
        "        self.output = nn.Linear(embedding_dim + embedding_dim // 2, 1)\n",
        "\n",
        "        # Init weights (Optional but recommended)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Embedding):\n",
        "                nn.init.normal_(m.weight, std=0.01)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        # GMF\n",
        "        gmf_u = self.gmf_user_embed(user)\n",
        "        gmf_i = self.gmf_item_embed(item)\n",
        "        gmf_vector = gmf_u * gmf_i # Element-wise product\n",
        "\n",
        "        # MLP\n",
        "        mlp_u = self.mlp_user_embed(user)\n",
        "        mlp_i = self.mlp_item_embed(item)\n",
        "        mlp_input = torch.cat([mlp_u, mlp_i], dim=-1)\n",
        "        mlp_vector = self.mlp(mlp_input)\n",
        "\n",
        "        # Concat & Predict\n",
        "        combined = torch.cat([gmf_vector, mlp_vector], dim=-1)\n",
        "        prediction = self.output(combined)\n",
        "\n",
        "        # Note: We don't use Sigmoid here because we use BCEWithLogitsLoss later\n",
        "        return prediction.squeeze()\n",
        "\n",
        "# Instantiate Model\n",
        "model = NeuMF(num_users, num_items, embedding_dim=32).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cdfb91f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdfb91f4",
        "outputId": "c1fa5f60-5a08-4dee-98df-6fc20d4a04d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 2538/2538 [02:08<00:00, 19.72it/s, loss=0.321]\n",
            "Epoch 2/5: 100%|██████████| 2538/2538 [02:08<00:00, 19.83it/s, loss=0.269]\n",
            "Epoch 3/5: 100%|██████████| 2538/2538 [02:08<00:00, 19.83it/s, loss=0.249]\n",
            "Epoch 4/5: 100%|██████████| 2538/2538 [02:08<00:00, 19.82it/s, loss=0.196]\n",
            "Epoch 5/5: 100%|██████████| 2538/2538 [02:07<00:00, 19.87it/s, loss=0.201]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "NeuMF Prediction: 100%|██████████| 23926/23926 [00:15<00:00, 1553.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeuMF] HR@50: 0.0675  NDCG@50: 0.0192\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.06745799548608208), np.float64(0.019187278734472554))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. Training Loop\n",
        "# ==========================================\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5 # For demo, you can increase to 10-20\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    # Progress bar for each epoch\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    for u, items, labels in pbar:\n",
        "        u, items, labels = u.to(device), items.to(device), labels.to(device)\n",
        "\n",
        "        # Flatten input for batch processing\n",
        "        # items shape: (batch, 1+num_neg) -> (batch * (1+num_neg))\n",
        "        # We repeat user '1+num_neg' times to match items\n",
        "        batch_size_curr = u.shape[0]\n",
        "        num_samples = items.shape[1]\n",
        "\n",
        "        u_flat = u.repeat_interleave(num_samples)\n",
        "        items_flat = items.view(-1)\n",
        "        labels_flat = labels.view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(u_flat, items_flat)\n",
        "        loss = criterion(preds, labels_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "print(\"Training Finished.\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. Generate Recommendations (Inference)\n",
        "# ==========================================\n",
        "model.eval()\n",
        "neumf_preds = {}\n",
        "test_users = test['user_idx'].unique()\n",
        "K = 50\n",
        "\n",
        "# For NeuMF, scoring every item for every user is SLOW.\n",
        "# Strategy:\n",
        "# 1. Compute user embeddings and item embeddings.\n",
        "# 2. But MLP makes it hard to do simple dot product.\n",
        "# 3. So we usually do Batch Inference on ALL items for each test user (or a candidate set).\n",
        "# Given we have 50k items, we can score all 50k for each test user.\n",
        "\n",
        "all_items = torch.arange(num_items, device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for u in tqdm(test_users, desc=\"NeuMF Prediction\"):\n",
        "        # Create input: user u repeated 50000 times\n",
        "        u_tensor = torch.tensor([u], device=device).repeat(num_items)\n",
        "\n",
        "        # Predict scores for all items\n",
        "        scores = model(u_tensor, all_items)\n",
        "\n",
        "        # Mask seen items (Optional but recommended)\n",
        "        # For simplicity in demo, we might skip masking or do it simple\n",
        "        # Here we just take Top-K directly\n",
        "\n",
        "        # Top-K\n",
        "        _, top_indices = torch.topk(scores, K)\n",
        "        neumf_preds[u] = top_indices.cpu().tolist()\n",
        "\n",
        "# Evaluate\n",
        "evaluate_model(\"NeuMF\", test, neumf_preds, K=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1a36776",
      "metadata": {
        "id": "e1a36776"
      },
      "source": [
        "## SASRec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c09da5da",
      "metadata": {
        "id": "c09da5da"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Dataset Class ---\n",
        "class SASRecDataset(Dataset):\n",
        "    def __init__(self, train_df, num_items, max_len=50):\n",
        "        \"\"\"\n",
        "        Initializes the SASRec Dataset, preparing fixed-length sequences for training.\n",
        "\n",
        "        Args:\n",
        "            train_df (pd.DataFrame): Training interactions DataFrame.\n",
        "            num_items (int): Total number of unique items (0 to num_items-1 are valid IDs).\n",
        "            max_len (int): Maximum sequence length for Transformer input.\n",
        "        \"\"\"\n",
        "        self.max_len = max_len\n",
        "        self.pad_id = num_items # Use num_items as the reserved ID for padding\n",
        "\n",
        "        # Convert training data into sorted item sequences per user\n",
        "        print(\"Preparing SASRec Sequences...\")\n",
        "        # Assume train_df is already sorted by 'user_idx' and 'ts'\n",
        "\n",
        "        # Extract sequences\n",
        "        sequences = train_df.groupby('user_idx')['item_idx'].apply(list)\n",
        "\n",
        "        self.user_ids = []\n",
        "        self.inputs = []  # Sequence input: [v1, v2, ..., v(t-1)]\n",
        "        self.targets = [] # Sequence target (next item prediction): [v2, v3, ..., v(t)]\n",
        "\n",
        "        # For each user, use the 'Leave-One-Out' concept for training sequences\n",
        "        for u_id, seq in tqdm(sequences.items(), desc=\"Processing Users\"):\n",
        "\n",
        "            # The last item is reserved for testing, so training uses up to the second-to-last item.\n",
        "            # However, for sequence modeling, we use the full history available in 'train'\n",
        "            # to generate (Input Seq, Target Seq) pairs for next-item prediction.\n",
        "\n",
        "            input_seq = seq[:-1] # E.g., [v1, v2, v3]\n",
        "            target_seq = seq[1:]  # E.g., [v2, v3, v4]\n",
        "\n",
        "            # Padding and Truncation\n",
        "            input_seq = input_seq[-self.max_len:]\n",
        "            target_seq = target_seq[-self.max_len:]\n",
        "\n",
        "            # Padding (Use self.pad_id for padding)\n",
        "            padding_len = self.max_len - len(input_seq)\n",
        "            padded_input = [self.pad_id] * padding_len + input_seq\n",
        "            padded_target = [self.pad_id] * padding_len + target_seq\n",
        "\n",
        "            # Truncate to max_len (already handled by slicing above, but keep for clarity)\n",
        "            self.inputs.append(padded_input[-self.max_len:])\n",
        "            self.targets.append(padded_target[-self.max_len:])\n",
        "            self.user_ids.append(u_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        # We model each user's history as ONE training sample\n",
        "        return len(self.user_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Returns: Input sequence (MaxLen), Target sequence (MaxLen), User ID (1)\n",
        "        return (torch.tensor(self.inputs[idx], dtype=torch.long),\n",
        "                torch.tensor(self.targets[idx], dtype=torch.long),\n",
        "                self.user_ids[idx])\n",
        "\n",
        "# Example instantiation:\n",
        "# train_dataset_sasrec = SASRecDataset(train, num_items=num_items, max_len=50)\n",
        "# train_loader_sasrec = DataLoader(train_dataset_sasrec, batch_size=256, shuffle=True)\n",
        "# print(\"SASRec Dataset Ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e04d1af2",
      "metadata": {
        "id": "e04d1af2"
      },
      "outputs": [],
      "source": [
        "# --- 2. Model Architecture ---\n",
        "class FeedForward(torch.nn.Module):\n",
        "    \"\"\"Point-wise Feed-Forward Network (FFN)\"\"\"\n",
        "    def __init__(self, hidden_size, dropout_rate=0.5):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(hidden_size, hidden_size, kernel_size=1)\n",
        "        self.dropout1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.conv2 = torch.nn.Conv1d(hidden_size, hidden_size, kernel_size=1)\n",
        "        self.dropout2 = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, MaxLen, HiddenSize) -> permute to (B, HiddenSize, MaxLen) for Conv1d\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.dropout1(self.relu(self.conv1(x)))\n",
        "        x = self.dropout2(self.conv2(x))\n",
        "        x = x.permute(0, 2, 1) # Back to (B, MaxLen, HiddenSize)\n",
        "        return x\n",
        "\n",
        "class AttentionBlock(torch.nn.Module):\n",
        "    \"\"\"Single Transformer Encoder Layer (Self-Attention + FFN)\"\"\"\n",
        "    def __init__(self, hidden_size, num_heads, dropout_rate=0.5):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        # PyTorch's MultiheadAttention includes Q, K, V linear layers internally\n",
        "        self.attn = torch.nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout_rate, batch_first=True)\n",
        "        self.ffn = FeedForward(hidden_size, dropout_rate)\n",
        "\n",
        "        self.layernorm1 = torch.nn.LayerNorm(hidden_size, eps=1e-6)\n",
        "        self.layernorm2 = torch.nn.LayerNorm(hidden_size, eps=1e-6)\n",
        "        self.dropout1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # 1. Self-Attention\n",
        "        # attn_output: (B, MaxLen, HiddenSize)\n",
        "        # key_padding_mask is not used here; mask is the causal mask\n",
        "        attn_output, _ = self.attn(x, x, x, attn_mask=mask)\n",
        "        x = self.layernorm1(x + self.dropout1(attn_output)) # Add & Norm\n",
        "\n",
        "        # 2. FFN\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.layernorm2(x + self.dropout2(ffn_output)) # Add & Norm\n",
        "        return x\n",
        "\n",
        "class SASRecModel(torch.nn.Module):\n",
        "    \"\"\"Self-Attentive Sequential Recommendation Model\"\"\"\n",
        "    def __init__(self, num_items, max_len, num_layers=2, num_heads=2, hidden_size=64, dropout_rate=0.2):\n",
        "        super(SASRecModel, self).__init__()\n",
        "\n",
        "        # Item embeddings (includes one extra slot for the PAD_ID)\n",
        "        self.item_embed = torch.nn.Embedding(num_items + 1, hidden_size, padding_idx=num_items)\n",
        "        self.position_embed = torch.nn.Embedding(max_len, hidden_size)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.num_items = num_items\n",
        "        self.max_len = max_len\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Transformer Encoder Stack\n",
        "        self.attention_blocks = torch.nn.ModuleList([\n",
        "            AttentionBlock(hidden_size, num_heads, dropout_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.layernorm = torch.nn.LayerNorm(hidden_size, eps=1e-6) # Final Layer Norm\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Initialize item and position embeddings\n",
        "        torch.nn.init.xavier_uniform_(self.item_embed.weight.data)\n",
        "        torch.nn.init.xavier_uniform_(self.position_embed.weight.data)\n",
        "\n",
        "    def forward(self, seq_in):\n",
        "        # seq_in: (B, MaxLen)\n",
        "\n",
        "        # 1. Embeddings and Positional Encoding\n",
        "        item_emb = self.item_embed(seq_in) # (B, MaxLen, HiddenSize)\n",
        "\n",
        "        # Create positional indices: [0, 1, ..., MaxLen-1]\n",
        "        positions = torch.arange(self.max_len, dtype=torch.long, device=seq_in.device)\n",
        "        pos_emb = self.position_embed(positions) # (MaxLen, HiddenSize)\n",
        "        pos_emb = pos_emb.unsqueeze(0).repeat(seq_in.size(0), 1, 1) # (B, MaxLen, HiddenSize)\n",
        "\n",
        "        x = item_emb + pos_emb\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # 2. Self-Attention Mask (Causality Mask)\n",
        "        # This mask prevents a token from attending to subsequent tokens (future information).\n",
        "        # Mask is upper triangular (disallowed to attend to future items).\n",
        "        # The mask shape should be (MaxLen, MaxLen)\n",
        "        attention_mask = torch.triu(torch.ones((self.max_len, self.max_len), dtype=torch.bool, device=seq_in.device), diagonal=1)\n",
        "        # MultiheadAttention expects False for not masked, True for masked.\n",
        "        # This mask should be applied as `attn_mask` in the attention layer.\n",
        "\n",
        "        # 3. Transformer Encoder Blocks\n",
        "        for block in self.attention_blocks:\n",
        "            x = block(x, mask=attention_mask)\n",
        "\n",
        "        # Final Layer Norm\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        # x: (B, MaxLen, HiddenSize) - Output sequence of latent vectors\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "09f46d19",
      "metadata": {
        "id": "09f46d19"
      },
      "outputs": [],
      "source": [
        "# --- 3. Training Loop ---\n",
        "def train_sasrec(model, train_loader, epochs, num_items, device):\n",
        "    \"\"\"\n",
        "    Trains the SASRec model using Cross-Entropy Loss with sequence data.\n",
        "    \"\"\"\n",
        "    # CrossEntropyLoss is used for multi-class classification (predicting the next item ID)\n",
        "    # ignore_index=num_items ensures that padding tokens do not contribute to the loss.\n",
        "    criterion = torch.nn.CrossEntropyLoss(ignore_index=num_items)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        pbar = tqdm(train_loader, desc=f\"SASRec Epoch {epoch+1}/{epochs}\")\n",
        "        total_loss = 0\n",
        "\n",
        "        for input_seq, target_seq, _ in pbar:\n",
        "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 1. Forward Pass: Get output latent vectors (B, MaxLen, HiddenSize)\n",
        "            latent_seq = model(input_seq)\n",
        "\n",
        "            # 2. Prediction Scores\n",
        "            # Scores for ALL items (Dot product with Item Embeddings is the prediction head)\n",
        "            # all_item_embeds: (NumItems+1, HiddenSize)\n",
        "            all_item_embeds = model.item_embed.weight\n",
        "\n",
        "            # scores: (B, MaxLen, NumItems+1)\n",
        "            scores = torch.matmul(latent_seq, all_item_embeds.transpose(0, 1))\n",
        "\n",
        "            # 3. Calculate Loss\n",
        "            # scores_flat: (B * MaxLen, NumItems+1)\n",
        "            scores_flat = scores.view(-1, scores.size(-1))\n",
        "            # targets_flat: (B * MaxLen)\n",
        "            targets_flat = target_seq.view(-1)\n",
        "\n",
        "            loss = criterion(scores_flat, targets_flat)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    print(\"SASRec Training Finished.\")\n",
        "\n",
        "# --- 4. Evaluation/Prediction Function ---\n",
        "def predict_sasrec(model, test_df, train_df, num_items, max_len, K=50):\n",
        "    \"\"\"\n",
        "    Generates Top-K recommendations for test users using the trained SASRec model (Full Ranking).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Extract each user's training history (needed for model input)\n",
        "    user_histories = train_df.groupby('user_idx')['item_idx'].apply(list).to_dict()\n",
        "\n",
        "    sasrec_preds = {}\n",
        "    test_users = test_df['user_idx'].unique()\n",
        "    pad_id = num_items\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for u in tqdm(test_users, desc=\"SASRec Prediction\"):\n",
        "\n",
        "            # a. Prepare input sequence (History S_u[:-1])\n",
        "            history = user_histories.get(u, [])\n",
        "            # The last item is the ground truth, so we use the history *before* that.\n",
        "            input_seq = history[:-1] # Input sequence to predict the last item (ground truth)\n",
        "            input_seq = input_seq[-max_len:] # Truncate\n",
        "\n",
        "            # b. Padding\n",
        "            padding_len = max_len - len(input_seq)\n",
        "            padded_input = [pad_id] * padding_len + input_seq\n",
        "\n",
        "            # c. Convert to Tensor (Batch size=1)\n",
        "            input_tensor = torch.tensor([padded_input[-max_len:]], dtype=torch.long, device=device)\n",
        "\n",
        "            # d. Forward Pass (1, MaxLen, HiddenSize)\n",
        "            latent_seq = model(input_tensor)\n",
        "\n",
        "            # e. Get User Representation h_t\n",
        "            # The prediction is based on the output at the last token position\n",
        "            last_index = max_len - 1\n",
        "            u_representation = latent_seq[0, last_index, :] # (HiddenSize)\n",
        "\n",
        "            # f. Scoring (Dot Product) against all item embeddings\n",
        "            # Exclude the PAD_ID embedding in the scoring\n",
        "            all_item_embeds = model.item_embed.weight[:-1]\n",
        "            scores = torch.matmul(u_representation, all_item_embeds.transpose(0, 1)) # (NumItems)\n",
        "\n",
        "            # g. Mask seen items (Crucial for sequential recommendation evaluation!)\n",
        "            # Items user u has already interacted with (must be excluded from recommendation)\n",
        "            seen_items = set(history[:-1])\n",
        "            mask = torch.ones_like(scores, dtype=torch.bool)\n",
        "\n",
        "            for i_idx in seen_items:\n",
        "                if i_idx < num_items:\n",
        "                    mask[i_idx] = False\n",
        "\n",
        "            # Set scores of seen items to a very low value to prevent their recommendation\n",
        "            scores[~mask] = -1e9\n",
        "\n",
        "            # h. Top-K\n",
        "            _, top_indices = torch.topk(scores, K)\n",
        "            sasrec_preds[u] = top_indices.cpu().tolist()\n",
        "\n",
        "    return sasrec_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "644f64ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "644f64ba",
        "outputId": "f659379d-c3c0-454c-cb5a-bc85f953be6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Items (NumItems): 50000\n",
            "SASRecModel(\n",
            "  (item_embed): Embedding(50001, 64, padding_idx=50000)\n",
            "  (position_embed): Embedding(50, 64)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (attention_blocks): ModuleList(\n",
            "    (0-1): 2 x AttentionBlock(\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (ffn): FeedForward(\n",
            "        (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
            "        (dropout1): Dropout(p=0.2, inplace=False)\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
            "        (dropout2): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (layernorm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
            "      (layernorm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.2, inplace=False)\n",
            "      (dropout2): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (layernorm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
            ")\n",
            "Preparing SASRec Sequences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Users: 23926it [00:00, 179406.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SASRec DataLoader Ready.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SASRec Epoch 1/20: 100%|██████████| 94/94 [00:04<00:00, 19.57it/s, loss=10.4]\n",
            "SASRec Epoch 2/20: 100%|██████████| 94/94 [00:04<00:00, 22.49it/s, loss=10.1]\n",
            "SASRec Epoch 3/20: 100%|██████████| 94/94 [00:04<00:00, 22.49it/s, loss=9.87]\n",
            "SASRec Epoch 4/20: 100%|██████████| 94/94 [00:04<00:00, 22.39it/s, loss=9.52]\n",
            "SASRec Epoch 5/20: 100%|██████████| 94/94 [00:04<00:00, 20.19it/s, loss=9.28]\n",
            "SASRec Epoch 6/20: 100%|██████████| 94/94 [00:04<00:00, 22.49it/s, loss=8.93]\n",
            "SASRec Epoch 7/20: 100%|██████████| 94/94 [00:04<00:00, 22.62it/s, loss=8.82]\n",
            "SASRec Epoch 8/20: 100%|██████████| 94/94 [00:04<00:00, 22.57it/s, loss=8.68]\n",
            "SASRec Epoch 9/20: 100%|██████████| 94/94 [00:04<00:00, 22.54it/s, loss=8.53]\n",
            "SASRec Epoch 10/20: 100%|██████████| 94/94 [00:04<00:00, 22.36it/s, loss=8.35]\n",
            "SASRec Epoch 11/20: 100%|██████████| 94/94 [00:04<00:00, 22.52it/s, loss=8.32]\n",
            "SASRec Epoch 12/20: 100%|██████████| 94/94 [00:04<00:00, 22.60it/s, loss=8.15]\n",
            "SASRec Epoch 13/20: 100%|██████████| 94/94 [00:04<00:00, 22.47it/s, loss=8.08]\n",
            "SASRec Epoch 14/20: 100%|██████████| 94/94 [00:04<00:00, 22.35it/s, loss=8.04]\n",
            "SASRec Epoch 15/20: 100%|██████████| 94/94 [00:04<00:00, 22.53it/s, loss=7.91]\n",
            "SASRec Epoch 16/20: 100%|██████████| 94/94 [00:04<00:00, 22.61it/s, loss=7.72]\n",
            "SASRec Epoch 17/20: 100%|██████████| 94/94 [00:04<00:00, 22.36it/s, loss=7.68]\n",
            "SASRec Epoch 18/20: 100%|██████████| 94/94 [00:04<00:00, 22.37it/s, loss=7.62]\n",
            "SASRec Epoch 19/20: 100%|██████████| 94/94 [00:04<00:00, 21.65it/s, loss=7.54]\n",
            "SASRec Epoch 20/20: 100%|██████████| 94/94 [00:04<00:00, 22.54it/s, loss=7.61]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SASRec Training Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SASRec Prediction: 100%|██████████| 23926/23926 [01:34<00:00, 253.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SASRec] HR@50: 0.1006  NDCG@50: 0.0309\n",
            "\n",
            "--- Summary ---\n",
            "[SASRec] HR@50: 0.1006  NDCG@50: 0.0309\n"
          ]
        }
      ],
      "source": [
        "# --- Hyperparameters ---\n",
        "max_len = 50           # Sequence length (must match the model definition)\n",
        "hidden_size = 64       # Embedding dimension\n",
        "epochs = 20             # Training epochs (can increase if needed)\n",
        "batch_size = 256\n",
        "K = 50                 # Top-K for evaluation\n",
        "\n",
        "# Ensure the SASRecModel and SASRecDataset classes are defined in previous cells\n",
        "\n",
        "print(f\"Total Items (NumItems): {num_items}\")\n",
        "\n",
        "# 1. Model Instantiation\n",
        "model_sasrec = SASRecModel(\n",
        "    num_items=num_items,\n",
        "    max_len=max_len,\n",
        "    hidden_size=hidden_size\n",
        ").to(device)\n",
        "\n",
        "print(model_sasrec)\n",
        "\n",
        "# 2. Dataset Instantiation (Using the corrected call with num_items)\n",
        "train_dataset_sasrec = SASRecDataset(train, num_items=num_items, max_len=max_len)\n",
        "\n",
        "# 3. DataLoader\n",
        "train_loader_sasrec = DataLoader(train_dataset_sasrec, batch_size=batch_size, shuffle=True)\n",
        "print(\"SASRec DataLoader Ready.\")\n",
        "\n",
        "# Ensure the train_sasrec function (from the previous answer) is defined in a cell above\n",
        "train_sasrec(model_sasrec, train_loader_sasrec, epochs=epochs, num_items=num_items, device=device)\n",
        "\n",
        "\n",
        "# Ensure the predict_sasrec function (from the previous answer) is defined in a cell above\n",
        "\n",
        "# 1. Inference/Prediction (Full Ranking)\n",
        "sasrec_preds = predict_sasrec(\n",
        "    model_sasrec,\n",
        "    test,\n",
        "    train,\n",
        "    num_items=num_items, # Pass num_items for PAD_ID calculation\n",
        "    max_len=max_len,\n",
        "    K=K\n",
        ")\n",
        "\n",
        "# 2. Evaluation\n",
        "hr_SR, ndcg_SR = evaluate_model(\"SASRec\", test, sasrec_preds, K=K)\n",
        "\n",
        "# Display final results\n",
        "print(\"\\n--- Summary ---\")\n",
        "print(f\"[SASRec] HR@{K}: {hr_SR:.4f}  NDCG@{K}: {ndcg_SR:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v1qurd_NaDRF"
      },
      "id": "v1qurd_NaDRF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}