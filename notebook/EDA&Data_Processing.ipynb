{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a9bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"../data/KuaiSAR_final\"\n",
    "\n",
    "# inter = pd.read_csv(data_path + '/rec_inter.csv')\n",
    "use_cols = ['user_id', 'item_id', 'timestamp', 'click', 'like', 'follow', 'search']\n",
    "df = pd.read_csv(data_path + '/rec_inter.csv', usecols=use_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d5068",
   "metadata": {},
   "source": [
    "## data cleaning & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c04fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning & preprocessing\n",
    "\n",
    "for c in [\"click\",\"like\",\"follow\",\"search\"]:\n",
    "    df[c] = df[c].fillna(0).astype(np.int8)\n",
    "\n",
    "# only keep recommendation interaction(none search)\n",
    "df = df[df['search'] == 0]\n",
    "\n",
    "df['pos'] = ((df['click'] + df['like'] + df['follow']) > 0).astype(np.int8)\n",
    "\n",
    "# timestamp\n",
    "ts = pd.to_numeric(df['timestamp'], errors='coerce')\n",
    "df = df[ts.notna()].copy()\n",
    "df['ts'] = ts.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c243c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 7461153 interactions\n",
      "Filtered: 2621974 interactions\n",
      "Users: 23936, Items: 50000\n",
      "Train: 2598038, Test: 23936\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Iterative Filtering (Core Stability)\n",
    "# ==========================================\n",
    "# Keep top K items to avoid OOM and reduce noise\n",
    "target_item_count = 50000\n",
    "min_user_inter = 5\n",
    "\n",
    "print(f\"Original: {len(df)} interactions\")\n",
    "\n",
    "# Filter Items first (Keep Top 50k)\n",
    "item_counts = df['item_id'].value_counts()\n",
    "if len(item_counts) > target_item_count:\n",
    "    top_items = item_counts.head(target_item_count).index\n",
    "    df_filtered = df[df['item_id'].isin(top_items)].copy()\n",
    "else:\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "# Filter Users (Keep >= 5 interactions)\n",
    "# We might need a loop because removing users might reduce item counts, and vice versa\n",
    "# But for simplicity, one pass usually works well enough for coursework\n",
    "user_counts = df_filtered['user_id'].value_counts()\n",
    "valid_users = user_counts[user_counts >= min_user_inter].index\n",
    "df_filtered = df_filtered[df_filtered['user_id'].isin(valid_users)].copy()\n",
    "\n",
    "print(f\"Filtered: {len(df_filtered)} interactions\")\n",
    "print(f\"Users: {df_filtered['user_id'].nunique()}, Items: {df_filtered['item_id'].nunique()}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. ID Remapping\n",
    "# ==========================================\n",
    "unique_users = df_filtered['user_id'].unique()\n",
    "unique_items = df_filtered['item_id'].unique()\n",
    "\n",
    "user2idx = {uid: i for i, uid in enumerate(unique_users)}\n",
    "item2idx = {iid: i for i, iid in enumerate(unique_items)}\n",
    "\n",
    "df_filtered['user_idx'] = df_filtered['user_id'].map(user2idx)\n",
    "df_filtered['item_idx'] = df_filtered['item_id'].map(item2idx)\n",
    "\n",
    "num_users = len(unique_users)\n",
    "num_items = len(unique_items)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Train/Test Split\n",
    "# ==========================================\n",
    "df_filtered = df_filtered.sort_values(['user_idx', 'ts'])\n",
    "grouped = df_filtered.groupby('user_idx')\n",
    "test = df_filtered.loc[grouped.tail(1).index]\n",
    "train = df_filtered.drop(test.index)\n",
    "\n",
    "print(f\"Train: {len(train)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5735b",
   "metadata": {},
   "source": [
    "## Most Popular Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6fc569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MostPopular] HR@50: 0.0304  NDCG@50: 0.0089\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Define Reusable Evaluation Function\n",
    "# ==========================================\n",
    "def evaluate_model(model_name, test_df, topk_preds, K=50):\n",
    "    \"\"\"\n",
    "    test_df: DataFrame with 'user_idx' and 'item_idx' (ground truth)\n",
    "    topk_preds: dict or Series, user_idx -> list of top K item_indices\n",
    "    \"\"\"\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    # Convert predictions to a dict for fast lookup if it isn't already\n",
    "    if not isinstance(topk_preds, dict):\n",
    "        pred_dict = topk_preds.to_dict()\n",
    "    else:\n",
    "        pred_dict = topk_preds\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        u = row['user_idx']\n",
    "        gt = row['item_idx']\n",
    "        \n",
    "        # Get recommendations for this user, default to empty if missing\n",
    "        recs = pred_dict.get(u, [])\n",
    "        \n",
    "        # HR@K\n",
    "        if gt in recs:\n",
    "            hits.append(1)\n",
    "            # NDCG@K\n",
    "            rank = recs.index(gt)\n",
    "            ndcgs.append(1.0 / np.log2(rank + 2))\n",
    "        else:\n",
    "            hits.append(0)\n",
    "            ndcgs.append(0.0)\n",
    "            \n",
    "    hr = np.mean(hits)\n",
    "    ndcg = np.mean(ndcgs)\n",
    "    print(f\"[{model_name}] HR@{K}: {hr:.4f}  NDCG@{K}: {ndcg:.4f}\")\n",
    "    return hr, ndcg\n",
    "\n",
    "# ==========================================\n",
    "# 2. Run Most Popular Baseline\n",
    "# ==========================================\n",
    "# Calculate popularity on TRAIN set only (avoid data leakage)\n",
    "# Using weighted popularity as you did before\n",
    "train['w'] = (1*train['click'] + 2*train['like'] + 3*train['follow']).astype(np.int16)\n",
    "pop_scores = train.groupby('item_idx')['w'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Get global Top-K list\n",
    "K = 50\n",
    "global_topk = pop_scores.index[:K].tolist()\n",
    "\n",
    "# Assign same topk to all test users\n",
    "most_pop_preds = {u: global_topk for u in test['user_idx'].unique()}\n",
    "\n",
    "# Evaluate\n",
    "hr_MP, ndcg_MP = evaluate_model(\"MostPopular\", test, most_pop_preds, K=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa83f2c",
   "metadata": {},
   "source": [
    "## Item CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba2df237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "Sparse User-Item Matrix Shape: torch.Size([23936, 50000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Setup Device (GPU/MPS/CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Prepare Indices and Values from Train Data\n",
    "indices = torch.tensor([train['user_idx'].values, train['item_idx'].values], dtype=torch.long)\n",
    "values = torch.tensor(train['w'].values, dtype=torch.float32)\n",
    "shape = torch.Size((num_users, num_items))\n",
    "\n",
    "\n",
    "# Construct User-Item Sparse Matrix\n",
    "# user_item_mat: (Users x Items)\n",
    "user_item_mat = torch.sparse_coo_tensor(indices, values, shape, device=device)\n",
    "print(f\"Sparse User-Item Matrix Shape: {user_item_mat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb0d195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0418, 0.0415,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0418, 0.0000, 0.0805,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0415, 0.0805, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Item-Item Similarity (Cosine)\n",
    "\n",
    "# Transpose to (Items x Users)\n",
    "item_user_mat = user_item_mat.t()\n",
    "\n",
    "item_user_dense = item_user_mat.to_dense()\n",
    "item_norms = torch.norm(item_user_dense, p=2, dim=1, keepdim=True)\n",
    "item_norms[item_norms == 0] = 1e-9\n",
    "item_user_norm = item_user_dense / item_norms\n",
    "sim_matrix = torch.mm(item_user_norm, item_user_norm.t()) # (Items x Items)\n",
    "\n",
    "sim_matrix.fill_diagonal_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b784c531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 120/120 [08:44<00:00,  4.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ItemCF_Torch] HR@50: 0.0547  NDCG@50: 0.0160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.05472927807486631, 0.01601873472111061)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Recommendations\n",
    "from tqdm import tqdm\n",
    "                                                         \n",
    "\n",
    "# 5. Generate Recommendations\n",
    "item_cf_preds = {}\n",
    "K = 50\n",
    "test_users_arr = test['user_idx'].unique()\n",
    "batch_size = 200 # Adjust based on memory\n",
    "\n",
    "for i in tqdm(range(0, len(test_users_arr), batch_size), desc=\"Predicting\"):\n",
    "    batch_uids = test_users_arr[i : i + batch_size]\n",
    "    batch_uids_tensor = torch.tensor(batch_uids, device=device)\n",
    "    \n",
    "    # Get History: (Batch x Items)\n",
    "    # index_select works on dense or sparse (if supported)\n",
    "    # converting batch history to dense for calculation\n",
    "    batch_hist = user_item_mat.index_select(0, batch_uids_tensor).to_dense()\n",
    "    \n",
    "    # Score: (Batch x Items) * (Items x Items) -> (Batch x Items)\n",
    "    scores = torch.mm(batch_hist, sim_matrix)\n",
    "    \n",
    "    # Mask seen items\n",
    "    scores = scores - 9999.0 * batch_hist\n",
    "    \n",
    "    # Top-K\n",
    "    _, topk_indices = torch.topk(scores, k=K, dim=1)\n",
    "    \n",
    "    # Store\n",
    "    topk_cpu = topk_indices.cpu().numpy()\n",
    "    for idx, u in enumerate(batch_uids):\n",
    "        item_cf_preds[u] = topk_cpu[idx].tolist()\n",
    "\n",
    "# 6. Evaluate\n",
    "hr_IC, ndcg_IC = evaluate_model(\"ItemCF\", test, item_cf_preds, K=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57243171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuMF Dataset Ready.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# ==========================================\n",
    "# 1. NeuMF Dataset with Negative Sampling\n",
    "# ==========================================\n",
    "class NeuMFDataset(Dataset):\n",
    "    def __init__(self, train_df, num_items, num_neg=4):\n",
    "        self.users = torch.tensor(train_df['user_idx'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(train_df['item_idx'].values, dtype=torch.long)\n",
    "        self.num_items = num_items\n",
    "        self.num_neg = num_neg\n",
    "        \n",
    "        # Pre-compute a set of interacted items for each user for fast negative sampling\n",
    "        # set lookup is O(1)\n",
    "        self.user_item_set = train_df.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Positive sample\n",
    "        u = self.users[idx]\n",
    "        i = self.items[idx]\n",
    "        \n",
    "        # Negative sampling\n",
    "        # We need to find an item 'j' that user 'u' has NOT seen\n",
    "        item_indices = []\n",
    "        labels = []\n",
    "        \n",
    "        # Add Positive\n",
    "        item_indices.append(i)\n",
    "        labels.append(1.0)\n",
    "        \n",
    "        # Add Negatives\n",
    "        interacted_items = self.user_item_set.get(u.item(), set())\n",
    "        \n",
    "        for _ in range(self.num_neg):\n",
    "            j = random.randint(0, self.num_items - 1)\n",
    "            # Simple rejection sampling\n",
    "            while j in interacted_items:\n",
    "                j = random.randint(0, self.num_items - 1)\n",
    "            \n",
    "            item_indices.append(torch.tensor(j, dtype=torch.long))\n",
    "            labels.append(0.0)\n",
    "            \n",
    "        return u, torch.stack(item_indices), torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "# num_neg=4 is standard for NCF/NeuMF\n",
    "train_dataset = NeuMFDataset(train, num_items=num_items, num_neg=4)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "print(\"NeuMF Dataset Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e94d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuMF(\n",
      "  (gmf_user_embed): Embedding(23936, 32)\n",
      "  (gmf_item_embed): Embedding(50000, 32)\n",
      "  (mlp_user_embed): Embedding(23936, 32)\n",
      "  (mlp_item_embed): Embedding(50000, 32)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (output): Linear(in_features=48, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. NeuMF Model Architecture\n",
    "# ==========================================\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=32):\n",
    "        super(NeuMF, self).__init__()\n",
    "        \n",
    "        # --- GMF Part ---\n",
    "        self.gmf_user_embed = nn.Embedding(num_users, embedding_dim)\n",
    "        self.gmf_item_embed = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # --- MLP Part ---\n",
    "        self.mlp_user_embed = nn.Embedding(num_users, embedding_dim)\n",
    "        self.mlp_item_embed = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # MLP Layers: Input(2*dim) -> dim -> dim/2 -> 1\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(embedding_dim, embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # --- Prediction Layer ---\n",
    "        # Concatenate GMF output (dim) and MLP output (dim/2)\n",
    "        self.output = nn.Linear(embedding_dim + embedding_dim // 2, 1)\n",
    "        \n",
    "        # Init weights (Optional but recommended)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        # GMF\n",
    "        gmf_u = self.gmf_user_embed(user)\n",
    "        gmf_i = self.gmf_item_embed(item)\n",
    "        gmf_vector = gmf_u * gmf_i # Element-wise product\n",
    "        \n",
    "        # MLP\n",
    "        mlp_u = self.mlp_user_embed(user)\n",
    "        mlp_i = self.mlp_item_embed(item)\n",
    "        mlp_input = torch.cat([mlp_u, mlp_i], dim=-1)\n",
    "        mlp_vector = self.mlp(mlp_input)\n",
    "        \n",
    "        # Concat & Predict\n",
    "        combined = torch.cat([gmf_vector, mlp_vector], dim=-1)\n",
    "        prediction = self.output(combined)\n",
    "        \n",
    "        # Note: We don't use Sigmoid here because we use BCEWithLogitsLoss later\n",
    "        return prediction.squeeze()\n",
    "\n",
    "# Instantiate Model\n",
    "model = NeuMF(num_users, num_items, embedding_dim=32).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdfb91f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 10149/10149 [02:02<00:00, 82.59it/s, loss=0.29] \n",
      "Epoch 2/5: 100%|██████████| 10149/10149 [01:51<00:00, 91.16it/s, loss=0.266]\n",
      "Epoch 3/5: 100%|██████████| 10149/10149 [01:53<00:00, 89.72it/s, loss=0.188]\n",
      "Epoch 4/5: 100%|██████████| 10149/10149 [01:49<00:00, 92.63it/s, loss=0.244] \n",
      "Epoch 5/5: 100%|██████████| 10149/10149 [01:53<00:00, 89.38it/s, loss=0.213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NeuMF Prediction: 100%|██████████| 23936/23936 [02:13<00:00, 179.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeuMF] HR@50: 0.0656  NDCG@50: 0.0186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.06563335561497326, 0.018584764755696204)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. Training Loop\n",
    "# ==========================================\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5 # For demo, you can increase to 10-20\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Progress bar for each epoch\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    for u, items, labels in pbar:\n",
    "        u, items, labels = u.to(device), items.to(device), labels.to(device)\n",
    "        \n",
    "        # Flatten input for batch processing\n",
    "        # items shape: (batch, 1+num_neg) -> (batch * (1+num_neg))\n",
    "        # We repeat user '1+num_neg' times to match items\n",
    "        batch_size_curr = u.shape[0]\n",
    "        num_samples = items.shape[1]\n",
    "        \n",
    "        u_flat = u.repeat_interleave(num_samples)\n",
    "        items_flat = items.view(-1)\n",
    "        labels_flat = labels.view(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(u_flat, items_flat)\n",
    "        loss = criterion(preds, labels_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "print(\"Training Finished.\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Generate Recommendations (Inference)\n",
    "# ==========================================\n",
    "model.eval()\n",
    "neumf_preds = {}\n",
    "test_users = test['user_idx'].unique()\n",
    "K = 50\n",
    "\n",
    "# For NeuMF, scoring every item for every user is SLOW.\n",
    "# Strategy:\n",
    "# 1. Compute user embeddings and item embeddings.\n",
    "# 2. But MLP makes it hard to do simple dot product.\n",
    "# 3. So we usually do Batch Inference on ALL items for each test user (or a candidate set).\n",
    "# Given we have 50k items, we can score all 50k for each test user.\n",
    "\n",
    "all_items = torch.arange(num_items, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for u in tqdm(test_users, desc=\"NeuMF Prediction\"):\n",
    "        # Create input: user u repeated 50000 times\n",
    "        u_tensor = torch.tensor([u], device=device).repeat(num_items)\n",
    "        \n",
    "        # Predict scores for all items\n",
    "        scores = model(u_tensor, all_items)\n",
    "        \n",
    "        # Mask seen items (Optional but recommended)\n",
    "        # For simplicity in demo, we might skip masking or do it simple\n",
    "        # Here we just take Top-K directly\n",
    "        \n",
    "        # Top-K\n",
    "        _, top_indices = torch.topk(scores, K)\n",
    "        neumf_preds[u] = top_indices.cpu().tolist()\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(\"NeuMF\", test, neumf_preds, K=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356c123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
